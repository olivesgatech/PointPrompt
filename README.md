# Abstract
The remarkable capabilities of the Segment Anything Model (SAM) for tackling image segmentation tasks in an intuitive and interactive manner has sparked interest in the design of effective visual prompts. Such interest has led to the creation of automated point prompt selection strategies, typically motivated from a feature extraction perspective. However, there is still very little understanding of how appropriate these automated visual prompting strategies are, particularly when compared to humans, across diverse image domains. Additionally, it is unknown whether interpretable factors like distance between the prompt points are related to high segmentation performance. To bridge these gaps, we introduce PointPrompt, the first visual segmentation prompting dataset with 198,590 sets of prompt points generated by 48 human annotators. Our prompting dataset is a comprehensive collection of prompts across 16 image categories from various domains (natural, underwater, medical and seismic), spanning multiple annotators per image. With PointPrompt, we also curate a set of human-interpretable features, such as prompt coverage and distance between prompt points, and provide a performance comparison against automated point selection baselines. 
Our statistical analysis and baseline comparisons not only highlight that humans perform consistently better than automated prompt sampling strategies, but also indicate that many features associated with the data and the utilized prompting approach are good predictors of segmentation performance. Overall, the PointPrompt dataset constitutes a valuable resource to accelerate the study and design of efficient visual prompting strategies. Further details along with the dataset links and codes are available at [this link](https://alregib.ece.gatech.edu/pointprompt-a-visual-prompting-dataset-based-on-the-segment-anything-model/).

# Dataset
Access the **dataset** [here](https://zenodo.org/records/11187949).
This dataset has two zip files: **Image datasets.zip** and **Prompting data.zip**.

**Image datasets.zip** contains all image datasets, along with ground truth labels. For each image dataset, there are 400 image-ground truth mask pairs. The image and ground truth masks are formatted as .npy arrays.
**Prompting data.zip** contains prompting data collected from human annotators. The structure appears as the following:

```
Prompting Results
├── Baseball bat                                 # Image dataset
    ├── st1                                      # Human annotator # 1
        ├── eachround                            # List of length t (number of timestamps); indicates which timesteps belong to each of the two rounds (if they exist)
        ├── masks                                # Contains the binary masks produced for each image, in format a_b_mask.png, where 'a' corresponds to the image number (0 to 399) and 'b' indexes through timestamps in the prompting process
        ├── points                               # Contains inclusion and exclusion points formatted as a_green.npy and a_red.npy respectively, where 'a' corresponds to the image number
        ├── scores                               # Contains the scores at each timestep for every image (mIoU)
        ├── sorts                                # Contains sorted timestamp indexes, going from max to min based on the score
    ├── st2                                      # Human annotator # 2 (same structure as st1)
        .
        .
        .
    ├── st3                                      # Human annotator # 3 (same structure as st1)
        .
        .
        .
.
.
.
├── Tie
```

# Code Usage

Please download the weights for SAM via:

```
!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth <br>
```

or from this [direct-link](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth) and place it in your repository folder. 

**Point Sampling Strategy Experiments:**
1. Go to the `point_sampling` directory
2. Make sure you read `Instructions.md`. If you wish to run the `Saliency` strategy, follow instructions to download the appropriate items.
3. The main script (``python3 main.py``) runs the code. You will need to alter `--img_dir` to the folder which keeps the image datasets (download above). You will also need to adjust `--results_dir` to where you wish for the results to be saved. Additionally, `--home_dir` should be the directory where the pretrained weights are located within the repository.
4. You can specify which strategy you wish to run by altering the `--query_strategy` parameter.

**Finetuning:**
For finetuning the structure of the dataset should be **altered** to look this way: 
```
Image datasets        # Image dataset
├── human             # the first prompting strategy. the human is a special case since it contains multiple prompt sets for each image
    ├── Baseball bat  # the dataset type            
        ├── st1_0_green.npy # the inclusion points for first user (st1) done on the first image (_0_) 
        ├── st1_0_red.npy # the exclusion points for first user (st1) done on the first image (_0_)
        ├── st1_1_green.npy # the inclusion points for first user (st1) done on the second image (_1_) 
        ├── st1_1_red.npy # the exclusion points for first user (st1) done on the second image (_1_) 
        .
        .
        .
        ├── st3_399_green.npy # the inclusion points for third user (st3) done on the 399 image (_399_) 
        ├── st3_399_red.npy # the exclusion points for third user (st3) done on the 399 image (_399_) 
    ├── Tie
├── entropy # the other prompting strategies do no have multiple prompting sets for each image.
    ├── Baseball bat  # the dataset type            
        ├── 0_green.npy # the inclusion points done on the first image (_0_) 
        ├── 0_red.npy # the exclusion points done on the first image (_0_)
        .
        .
        .
        ├── 399_green.npy # the inclusion points done on the 399 image (_399_) 
        ├── 399_red.npy # the exclusion points done on the 399 image (_399_) 
    ├── Tie
.
.
.
├── samples
    ├── Baseball bat  # the dataset type            
        ├── 0_sample.npy # the first image in the Baseball bat dataset 
        ├── 1_sample.npy # the second image in the Baseball bat dataset 
        .
        .
        .
        ├── 399_sample.npy # the 399th image in the Baseball bat dataset
    ├── Tie
├── labels
    ├── Baseball bat  # the dataset type            
        ├── 0_label.npy # the first ground truth mask in the Baseball bat dataset 
        ├── 1_label.npy # the second ground truth mask in the Baseball bat dataset 
        .
        .
        .
        ├── 399_label.npy # the 399th ground truth mask in the Baseball bat dataset
    ├── Tie
```
So the Image datasets folder contain the prompting strategies files, also the images files under samples, and labels (ground truth) file.

1- Download the requirements: 
```
pip install -r requirements.txt
```
2- To finetune the prompt encoder based on each prompting method. The weights will be saved by the strategy name: 
```
python train.py --path "Image datasets" --train_ratio 0.8 --path_to_sam "sam_vit_h_4b8939.pth" 
```
3- Test each model on all data and save the results in an excel sheet.
```
python inference.py --path "Image datasets" 
```
# Links

Automated point selection strategies:

[SAMAug](https://github.com/yhydhx/SAMAug)

[SAM-PT](https://github.com/SysCV/sam-pt)

# Citation
