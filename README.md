# Abstract
The remarkable capabilities of the Segment Anything Model (SAM) for tackling image segmentation tasks in an intuitive and interactive manner has sparked interest in the design of effective visual prompts. Such interest has led to the creation of automated point prompt selection strategies, typically motivated from a feature extraction perspective. However, there is still very little understanding of how appropriate these automated visual prompting strategies are, particularly when compared to humans, across diverse image domains. Additionally, it is unknown whether interpretable factors like distance between the prompt points are related to high segmentation performance. To bridge these gaps, we introduce PointPrompt, the first visual segmentation prompting dataset with 198,590 sets of prompt points generated by 48 human annotators. Our prompting dataset is a comprehensive collection of prompts across 16 image categories from various domains (natural, underwater, medical and seismic), spanning multiple annotators per image. With PointPrompt, we also curate a set of human-interpretable features, such as prompt coverage and distance between prompt points, and provide a performance comparison against automated point selection baselines. 
Our statistical analysis and baseline comparisons not only highlight that humans perform consistently better than automated prompt sampling strategies, but also indicate that many features associated with the data and the utilized prompting approach are good predictors of segmentation performance. Overall, the PointPrompt dataset constitutes a valuable resource to accelerate the study and design of efficient visual prompting strategies. Further details along with the dataset links and codes are available at 
# Dataset
# Code Usage
# Links
# Citation
